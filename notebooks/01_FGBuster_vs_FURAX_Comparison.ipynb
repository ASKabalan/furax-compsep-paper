{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FGBuster vs FURAX: Framework Comparison for CMB Component Separation\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/CMBSciPol/furax-compsep-paper/blob/main/notebooks/01_FGBuster_vs_FURAX_Comparison.ipynb)\n",
    "\n",
    "## ðŸŽ¯ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- âœ… Understand the differences between traditional (FGBuster) and modern (FURAX) component separation frameworks\n",
    "- âœ… See the performance advantages of JAX over NumPy for CMB analysis\n",
    "- âœ… Learn how to implement and benchmark likelihood functions\n",
    "- âœ… Understand automatic differentiation benefits for parameter optimization\n",
    "\n",
    "## ðŸ“š Background\n",
    "\n",
    "### The Component Separation Problem\n",
    "\n",
    "CMB observations contain multiple astrophysical components:\n",
    "- **CMB signal**: What we want to measure\n",
    "- **Galactic dust**: Modified blackbody emission\n",
    "- **Synchrotron**: Power-law emission from cosmic rays\n",
    "- **Instrumental noise**: Detector and systematic effects\n",
    "\n",
    "The challenge is to separate these components accurately to recover the CMB signal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"EQX_ON_ERRORS\"] = \"nan\"\n",
    "# Core scientific computing\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from functools import partial\n",
    "import operator\n",
    "\n",
    "# JAX for high-performance computing\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jaxopt\n",
    "\n",
    "# FGBuster - Traditional component separation framework\n",
    "from fgbuster import (\n",
    "    CMB, Dust, Synchrotron,\n",
    "    basic_comp_sep, get_instrument,\n",
    ")\n",
    "\n",
    "# FURAX - Modern JAX-based framework\n",
    "from furax import HomothetyOperator\n",
    "from furax.obs.landscapes import Stokes\n",
    "from furax.obs.operators import (\n",
    "    CMBOperator, DustOperator, MixingMatrixOperator, SynchrotronOperator,\n",
    ")\n",
    "\n",
    "# Set JAX to use 64-bit precision for scientific accuracy\n",
    "jax.config.update(\"jax_enable_x64\", True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŒŒ Step 1: Generate Simulated Sky Maps\n",
    "\n",
    "We start by creating realistic CMB and foreground simulations using PySM (Python Sky Model). These simulated observations will serve as our test data for comparing the two frameworks.\n",
    "\n",
    "### Key Parameters:\n",
    "- **NSIDE = 32**: HEALPix resolution (creates 12,288 pixels)\n",
    "- **Instrument**: LiteBIRD frequency configuration (15 bands: 40-402 GHz)\n",
    "- **Components**: CMB + dust + synchrotron emission\n",
    "- **Stokes**: I, Q, U polarization parameters\n",
    "\n",
    "### Why Use Simulations?\n",
    "1. **Ground truth**: We know the input parameters\n",
    "2. **Controlled testing**: Compare framework accuracy\n",
    "3. **Reproducibility**: Same data for fair comparison\n",
    "\n",
    "> **ðŸ’¡ Pro Tip**: On HPC clusters without internet access, these maps are pre-cached using `generate_maps.py`\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded freq_maps for nside 32 from cache and noise False.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../data\")\n",
    "from generate_maps import save_to_cache\n",
    "\n",
    "nsides = [32]\n",
    "for nside in nsides:\n",
    "    save_to_cache(nside)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded freq_maps for nside 32 from cache.\n",
      "freq_maps shape: (15, 3, 12288)\n"
     ]
    }
   ],
   "source": [
    "from generate_maps import load_from_cache\n",
    "\n",
    "nside = 32\n",
    "\n",
    "nu, freq_maps = load_from_cache(nside)\n",
    "# Check the shape of freq_maps\n",
    "print(\"freq_maps shape:\", freq_maps.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furax expects a `Stokes` object as input, so we convert the frequency maps to the correct format, so we transform the `freq_maps` into a `Stokes` object to make it compatible with Furax.\n",
    "\n",
    "__Note__: Although Furax includes its own functions to create sky maps from PySM, we use `fgbuster` here to ensure that both methods receive identical inputs for comparison.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StokesIQU(i=ShapeDtypeStruct(shape=(15, 12288), dtype=float64), q=ShapeDtypeStruct(shape=(15, 12288), dtype=float64), u=ShapeDtypeStruct(shape=(15, 12288), dtype=float64))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = Stokes.from_stokes(I=freq_maps[:, 0, :], Q=freq_maps[:, 1, :], U=freq_maps[:, 2, :])\n",
    "d.structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dust_nu0 = 150.0\n",
    "synchrotron_nu0 = 20.0\n",
    "instrument = get_instrument(\"LiteBIRD\")\n",
    "components = [CMB(), Dust(dust_nu0), Synchrotron(synchrotron_nu0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Likelihood Function for Component Separation\n",
    "\n",
    "In this cell, we define the `negative_log_prob` function, which calculates the negative log-likelihood of observing the given data `d` based on the model parameters.\n",
    "\n",
    "The likelihood function is based on a quadratic form that includes the mixing matrix `A`, inverse noise covariance `N^{-1}`, and observed data `d`. The key term in the likelihood is:\n",
    "\n",
    "$$\n",
    "\\left(A^T N^{-1} d\\right)^T \\left(A^T N^{-1} A\\right)^{-1} \\left(A^T N^{-1} d\\right)\n",
    "$$\n",
    "\n",
    "### Explanation of Each Term\n",
    "\n",
    "1. **$A$**: The mixing matrix operator, which maps the component space to the observed frequency space.\n",
    "2. **$N^{-1}$**: The inverse of the noise covariance matrix, represented by `invN` in the code.\n",
    "3. **$d$**: The observed data, which is structured as a `Stokes` in Furax.\n",
    "\n",
    "### Implementation Details\n",
    "\n",
    "- **Transposing and Applying `A`**: `A.T(d)` applies the transpose of `A` to `d`, equivalent to the term $A^T d$.\n",
    "- **Computing the Likelihood**: The quadratic form is computed by applying $A^T N^{-1} d$, inverting $A^T N^{-1} A$, and performing matrix multiplications to evaluate the likelihood.\n",
    "- **Negative Log-Likelihood**: The final output of `negative_log_prob` is the negative of this log-likelihood value, allowing us to use it as a loss function for optimization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "invN = HomothetyOperator(jnp.ones(1), _in_structure=d.structure)\n",
    "DND = invN(d) @ d\n",
    "\n",
    "in_structure = d.structure_for((d.shape[1],))\n",
    "best_params = {\"temp_dust\": 20.0, \"beta_dust\": 1.54, \"beta_pl\": -3.0}\n",
    "\n",
    "dust_nu0 = 150.0\n",
    "synchrotron_nu0 = 20.0\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def negative_log_prob(params, d):\n",
    "    cmb = CMBOperator(nu, in_structure=in_structure)\n",
    "    dust = DustOperator(\n",
    "        nu,\n",
    "        frequency0=dust_nu0,\n",
    "        temperature=params[\"temp_dust\"],\n",
    "        beta=params[\"beta_dust\"],\n",
    "        in_structure=in_structure,\n",
    "    )\n",
    "    synchrotron = SynchrotronOperator(\n",
    "        nu,\n",
    "        frequency0=synchrotron_nu0,\n",
    "        beta_pl=params[\"beta_pl\"],\n",
    "        in_structure=in_structure,\n",
    "    )\n",
    "\n",
    "    A = MixingMatrixOperator(cmb=cmb, dust=dust, synchrotron=synchrotron)\n",
    "\n",
    "    x = (A.T @ invN)(d)\n",
    "    likelihood = jax.tree.map(lambda a, b: a @ b, x, (A.T @ invN @ A).I(x))\n",
    "    summed_log_prob = jax.tree.reduce(operator.add, likelihood)\n",
    "\n",
    "    return -summed_log_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the performance of the likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance of the nll evaluation\n",
      "23.1 ms Â± 4.34 ms per loop (mean Â± std. dev. of 7 runs, 10 loops each)\n",
      "Performance of the nll grad evaluation\n",
      "63.8 ms Â± 599 Î¼s per loop (mean Â± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "print(\"Performance of the nll evaluation\")\n",
    "negative_log_prob(best_params, d).block_until_ready()\n",
    "%timeit negative_log_prob(best_params, d).block_until_ready()\n",
    "print(\"Performance of the nll grad evaluation\")\n",
    "jax.grad(negative_log_prob)(best_params, d)[\"beta_pl\"].block_until_ready()\n",
    "%timeit jax.grad(negative_log_prob)(best_params, d)['beta_pl'].block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for Correctness\n",
    "\n",
    "In this cell, we perform a basic correctness check by comparing the gradients of the negative log-likelihood at two sets of parameters:\n",
    "\n",
    "1. **Wrong Parameters**: A set of parameters obtained by adding random noise to `best_params`.\n",
    "2. **Correct Parameters**: The original `best_params`.\n",
    "\n",
    "By calculating and comparing the gradient magnitudes (using the `max` reduction), we can verify that the gradient at the correct parameters is smaller, indicating proximity to an optimal or near-optimal point.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong parameters grad -113186444.45561369\n",
      "Correct parameters grad 0.0027135219067152814\n"
     ]
    }
   ],
   "source": [
    "wrong_params = jax.tree.map(lambda x: x + jax.random.normal(jax.random.PRNGKey(0)), best_params)\n",
    "print(f\"Wrong parameters grad {jax.tree.reduce(max, jax.grad(negative_log_prob)(wrong_params, d))}\")\n",
    "print(\n",
    "    f\"Correct parameters grad {jax.tree.reduce(max, jax.grad(negative_log_prob)(best_params, d))}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Off the shelf likelihoods\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from furax.obs import negative_log_likelihood\n",
    "\n",
    "negative_log_likelihood = partial(\n",
    "    negative_log_likelihood, dust_nu0=dust_nu0, synchrotron_nu0=synchrotron_nu0\n",
    ")\n",
    "\n",
    "L = negative_log_likelihood(best_params, nu=nu, N=invN, d=d)\n",
    "\n",
    "assert jax.tree.all(\n",
    "    jax.tree.map(lambda x, y: jnp.isclose(x, y, rtol=1e-5), L, negative_log_prob(best_params, d))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validating Against FGBuster: The `c1d0s0` Model\n",
    "\n",
    "In this section, we validate our custom implementation of the likelihood model by comparing it to the `c1d0s0` model from `fgbuster`. By aligning our results with FGBusterâ€™s well-established component separation model, we ensure that our setup and computations are consistent and reliable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 1 : Initial Validation: Using `best_params` as the Starting Point\n",
    "\n",
    "We begin the validation process by setting `best_params` as the initial point for both our custom implementation and FGBusterâ€™s `c1d0s0` model. This allows us to directly compare the outputs and confirm that the models produce similar results when initialized wit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dust.beta_d', 'Dust.temp', 'Synchrotron.beta_pl']\n",
      "[ 1.54 20.   -3.  ]\n"
     ]
    }
   ],
   "source": [
    "components[1]._set_default_of_free_symbols(beta_d=1.54, temp=20.0)\n",
    "components[2]._set_default_of_free_symbols(beta_pl=-3.0)\n",
    "\n",
    "result = basic_comp_sep(components, instrument, freq_maps)\n",
    "print(result.params)\n",
    "print(result.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wassim/micromamba/envs/fg/lib/python3.11/site-packages/jaxopt/_src/scipy_wrappers.py:341: OptimizeWarning: Unknown solver options: maxiter\n",
      "  res = osp.optimize.minimize(scipy_fun, jnp_to_onp(init_params, self.dtype),\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'beta_dust': Array(1.53999953, dtype=float64),\n",
       " 'beta_pl': Array(-2.99999996, dtype=float64),\n",
       " 'temp_dust': Array(19.99999945, dtype=float64)}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params = {\"temp_dust\": 20.0, \"beta_dust\": 1.54, \"beta_pl\": -3.0}\n",
    "\n",
    "scipy_solver = jaxopt.ScipyMinimize(fun=negative_log_likelihood, method=\"TNC\", jit=True, tol=1e-10)\n",
    "result = scipy_solver.run(best_params, nu=nu, N=invN, d=d)\n",
    "result.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 2 : Validation with Incorrect Parameter: Setting `beta_dust` to a Wrong Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<lambdifygenerated-9>:2: RuntimeWarning: overflow encountered in power\n",
      "  return 568.8620443215493*(0.05*nu)**beta_pl*numpy.expm1(0.01760867023799751*nu)**2*exp(-0.01760867023799751*nu)/(nu**2*numpy.expm1(0.3521734047599502)**2)\n",
      "<lambdifygenerated-9>:2: RuntimeWarning: overflow encountered in power\n",
      "  return 568.8620443215493*(0.05*nu)**beta_pl*numpy.expm1(0.01760867023799751*nu)**2*exp(-0.01760867023799751*nu)/(nu**2*numpy.expm1(0.3521734047599502)**2)\n",
      "<lambdifygenerated-10>:2: RuntimeWarning: overflow encountered in power\n",
      "  return 568.8620443215493*(0.05*nu)**beta_pl*numpy.expm1(0.01760867023799751*nu)**2*exp(-0.01760867023799751*nu)*log(0.05*nu)/(nu**2*numpy.expm1(0.3521734047599502)**2)\n",
      "<lambdifygenerated-9>:2: RuntimeWarning: overflow encountered in multiply\n",
      "  return 568.8620443215493*(0.05*nu)**beta_pl*numpy.expm1(0.01760867023799751*nu)**2*exp(-0.01760867023799751*nu)/(nu**2*numpy.expm1(0.3521734047599502)**2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVD of A failed -> logL = -inf\n",
      "SVD of A failed -> logL_dB not updated\n",
      "SVD of A failed -> logL = -inf\n",
      "SVD of A failed -> logL = -inf\n",
      "SVD of A failed -> logL = -inf\n",
      "SVD of A failed -> logL = -inf\n",
      "SVD of A failed -> logL = -inf\n",
      "SVD of A failed -> logL = -inf\n",
      "['Dust.beta_d', 'Dust.temp', 'Synchrotron.beta_pl']\n",
      "[ 1.53194524 19.97377854 -2.9430962 ]\n"
     ]
    }
   ],
   "source": [
    "components[1]._set_default_of_free_symbols(beta_d=2.54, temp=20.0)\n",
    "components[2]._set_default_of_free_symbols(beta_pl=-3.0)\n",
    "\n",
    "result = basic_comp_sep(components, instrument, freq_maps)\n",
    "print(result.params)\n",
    "print(result.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wassim/micromamba/envs/fg/lib/python3.11/site-packages/jaxopt/_src/scipy_wrappers.py:341: OptimizeWarning: Unknown solver options: maxiter\n",
      "  res = osp.optimize.minimize(scipy_fun, jnp_to_onp(init_params, self.dtype),\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'beta_dust': Array(1.53991301, dtype=float64),\n",
       " 'beta_pl': Array(-2.99768003, dtype=float64),\n",
       " 'temp_dust': Array(20.00280906, dtype=float64)}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {\"temp_dust\": 20.0, \"beta_dust\": 2.54, \"beta_pl\": -3.0}\n",
    "\n",
    "scipy_solver = jaxopt.ScipyMinimize(fun=negative_log_likelihood, method=\"TNC\", jit=True, tol=1e-10)\n",
    "result = scipy_solver.run(params, nu=nu, N=invN, d=d)\n",
    "result.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 3 : Setting `beta_dust` and `beta_pl` to Incorrect Values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<lambdifygenerated-9>:2: RuntimeWarning: overflow encountered in power\n",
      "  return 568.8620443215493*(0.05*nu)**beta_pl*numpy.expm1(0.01760867023799751*nu)**2*exp(-0.01760867023799751*nu)/(nu**2*numpy.expm1(0.3521734047599502)**2)\n",
      "<lambdifygenerated-9>:2: RuntimeWarning: overflow encountered in power\n",
      "  return 568.8620443215493*(0.05*nu)**beta_pl*numpy.expm1(0.01760867023799751*nu)**2*exp(-0.01760867023799751*nu)/(nu**2*numpy.expm1(0.3521734047599502)**2)\n",
      "<lambdifygenerated-10>:2: RuntimeWarning: overflow encountered in power\n",
      "  return 568.8620443215493*(0.05*nu)**beta_pl*numpy.expm1(0.01760867023799751*nu)**2*exp(-0.01760867023799751*nu)*log(0.05*nu)/(nu**2*numpy.expm1(0.3521734047599502)**2)\n",
      "<lambdifygenerated-9>:2: RuntimeWarning: overflow encountered in multiply\n",
      "  return 568.8620443215493*(0.05*nu)**beta_pl*numpy.expm1(0.01760867023799751*nu)**2*exp(-0.01760867023799751*nu)/(nu**2*numpy.expm1(0.3521734047599502)**2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVD of A failed -> logL = -inf\n",
      "SVD of A failed -> logL_dB not updated\n",
      "SVD of A failed -> logL = -inf\n",
      "SVD of A failed -> logL = -inf\n",
      "SVD of A failed -> logL = -inf\n",
      "SVD of A failed -> logL = -inf\n",
      "SVD of A failed -> logL = -inf\n",
      "SVD of A failed -> logL = -inf\n",
      "['Dust.beta_d', 'Dust.temp', 'Synchrotron.beta_pl']\n",
      "[ 1.53034346 19.97418092 -5.99479857]\n"
     ]
    }
   ],
   "source": [
    "components[1]._set_default_of_free_symbols(beta_d=2.54, temp=20.0)\n",
    "components[2]._set_default_of_free_symbols(beta_pl=-6.0)\n",
    "\n",
    "result = basic_comp_sep(components, instrument, freq_maps)\n",
    "print(result.params)\n",
    "print(result.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wassim/micromamba/envs/fg/lib/python3.11/site-packages/jaxopt/_src/scipy_wrappers.py:341: OptimizeWarning: Unknown solver options: maxiter\n",
      "  res = osp.optimize.minimize(scipy_fun, jnp_to_onp(init_params, self.dtype),\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'beta_dust': Array(1.53998646, dtype=float64),\n",
       " 'beta_pl': Array(-2.99989118, dtype=float64),\n",
       " 'temp_dust': Array(20.00048047, dtype=float64)}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {\"temp_dust\": 20.0, \"beta_dust\": 2.54, \"beta_pl\": -6.0}\n",
    "\n",
    "scipy_solver = jaxopt.ScipyMinimize(fun=negative_log_likelihood, method=\"TNC\", jit=True, tol=1e-10)\n",
    "result = scipy_solver.run(params, nu=nu, N=invN, d=d)\n",
    "result.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 4 : Setting All Parameters to Incorrect Values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dust.beta_d', 'Dust.temp', 'Synchrotron.beta_pl']\n",
      "[ 1.54 20.   -3.  ]\n"
     ]
    }
   ],
   "source": [
    "components[1]._set_default_of_free_symbols(beta_d=2.54, temp=25.0)\n",
    "components[2]._set_default_of_free_symbols(beta_pl=-6.0)\n",
    "\n",
    "result = basic_comp_sep(components, instrument, freq_maps)\n",
    "print(result.params)\n",
    "print(result.x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Optax optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optax\n",
    "import optax.tree_utils as otu\n",
    "from jax_grid_search import optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final parameters: {'beta_dust': Array(1.53999998, dtype=float64), 'beta_pl': Array(-2.99999997, dtype=float64), 'temp_dust': Array(20.00000052, dtype=float64)}, number of evaluations: 39\n",
      "Initial Value: -6114624700347.492\n"
     ]
    }
   ],
   "source": [
    "solver = optax.lbfgs()\n",
    "\n",
    "params = {\"temp_dust\": 25.0, \"beta_dust\": 2.54, \"beta_pl\": -6.0}\n",
    "\n",
    "\n",
    "final_params, final_state = optimize(\n",
    "    params, negative_log_likelihood, solver, max_iter=100, tol=1e-4, nu=nu, N=invN, d=d\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Final parameters: {final_params}, number of evaluations: {otu.tree_get(final_state, 'count')}\"\n",
    ")\n",
    "print(f\"Initial Value: {negative_log_prob(final_params, d=d)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
